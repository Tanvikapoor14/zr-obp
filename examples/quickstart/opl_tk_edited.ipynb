{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Quickstart Example with Off-Policy Learners\n",
    "---\n",
    "This notebook provides an example of implementing several off-policy learning methods with synthetic logged bandit data.\n",
    "\n",
    "The example consists of the following four major steps:\n",
    "- (1) Generating Synthetic Data\n",
    "- (2) Off-Policy Learning\n",
    "- (3) Evaluation of Off-Policy Learners\n",
    "\n",
    "Please see [../examples/opl](../opl) for a more sophisticated example of the evaluation of off-policy learners with synthetic bandit data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "# needed when using Google Colab\n",
    "# !pip install obp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RandomForest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "#sys.path.append('/Users/tanvikapoor/zr-obp')\n",
    "print(sys.path)\n",
    "\n",
    "# import open bandit pipeline (obp)\n",
    "import obp\n",
    "from obp.dataset import (\n",
    "    SyntheticBanditDataset,\n",
    "    logistic_reward_function,\n",
    "    linear_reward_function\n",
    ")\n",
    "from obp.policy import (\n",
    "    IPWLearner, \n",
    "    QLearner,\n",
    "    NNPolicyLearner, \n",
    "    Random\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['/Users/tanvikapoor/zr-obp/examples/quickstart', '/Users/tanvikapoor/.vscode/extensions/ms-toolsai.jupyter-2021.8.1236758218/pythonFiles/vscode_datascience_helpers', '/Users/tanvikapoor/.vscode/extensions/ms-toolsai.jupyter-2021.8.1236758218/pythonFiles', '/Users/tanvikapoor/.vscode/extensions/ms-toolsai.jupyter-2021.8.1236758218/pythonFiles/lib/python', '/Users/tanvikapoor/opt/anaconda3/lib/python38.zip', '/Users/tanvikapoor/opt/anaconda3/lib/python3.8', '/Users/tanvikapoor/opt/anaconda3/lib/python3.8/lib-dynload', '', '/Users/tanvikapoor/.local/lib/python3.8/site-packages', '/Users/tanvikapoor/opt/anaconda3/lib/python3.8/site-packages', '/Users/tanvikapoor/opt/anaconda3/lib/python3.8/site-packages/aeosa', '/Users/tanvikapoor/opt/anaconda3/lib/python3.8/site-packages/IPython/extensions', '/Users/tanvikapoor/.ipython', '/Users/tanvikapoor/zr-obp', '/Users/tanvikapoor/zr-obp']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# obp version\n",
    "print(obp.__version__)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.5.2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (1) Generating Synthetic Data\n",
    "`obp.dataset.SyntheticBanditDataset` is an easy-to-use synthetic data generator.\n",
    "\n",
    "It takes \n",
    "- number of actions (`n_actions`, $|\\mathcal{A}|$)\n",
    "- dimension of context vectors (`dim_context`, $d$)\n",
    "- reward function (`reward_function`, $q(x,a)=\\mathbb{E}[r|x,a]$)\n",
    "\n",
    "as inputs and generates synthetic logged bandit data that can be used to evaluate the performance of decision making policies (obtained by `off-policy learning`)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# generate synthetic logged bandit data with 10 actions\n",
    "# we use `logistic function` as the reward function and control the behavior policy with `beta`\n",
    "# one can define their own reward function and behavior policy function such as nonlinear ones. \n",
    "dataset = SyntheticBanditDataset(\n",
    "    n_actions=10,\n",
    "    dim_context=5,\n",
    "    beta=-2, # inverse temperature parameter to control the optimality and entropy of the behavior policy\n",
    "    reward_type=\"binary\", # \"binary\" or \"continuous\"\n",
    "    reward_function=logistic_reward_function,\n",
    "    random_state=12345,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# obtain training and test sets of synthetic logged bandit data\n",
    "n_rounds_train, n_rounds_test = 10000, 10000\n",
    "bandit_feedback_train = dataset.obtain_batch_bandit_feedback(n_rounds=n_rounds_train)\n",
    "bandit_feedback_test = dataset.obtain_batch_bandit_feedback(n_rounds=n_rounds_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "the logged bandit dataset is collected by the behavior policy as follows.\n",
    "\n",
    "$ \\mathcal{D}_b := \\{(x_i,a_i,r_i)\\}_{i=1}^n$  where $(x,a,r) \\sim p(x)\\pi_b(a | x)p(r | x,a) $"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# `bandit_feedback` is a dictionary storing synthetic logged bandit data\n",
    "bandit_feedback_train"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'n_rounds': 10000,\n",
       " 'n_actions': 10,\n",
       " 'context': array([[-0.20470766,  0.47894334, -0.51943872, -0.5557303 ,  1.96578057],\n",
       "        [ 1.39340583,  0.09290788,  0.28174615,  0.76902257,  1.24643474],\n",
       "        [ 1.00718936, -1.29622111,  0.27499163,  0.22891288,  1.35291684],\n",
       "        ...,\n",
       "        [-1.27028221,  0.80914602, -0.45084222,  0.47179511,  1.89401115],\n",
       "        [-0.68890924,  0.08857502, -0.56359347, -0.41135069,  0.65157486],\n",
       "        [ 0.51204121,  0.65384817, -1.98849253, -2.14429131, -0.34186901]]),\n",
       " 'action_context': array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]),\n",
       " 'action': array([9, 2, 1, ..., 0, 3, 7]),\n",
       " 'position': None,\n",
       " 'reward': array([1, 0, 0, ..., 0, 0, 1]),\n",
       " 'expected_reward': array([[0.81612381, 0.62585527, 0.3867853 , ..., 0.62527072, 0.58635322,\n",
       "         0.38638404],\n",
       "        [0.52901819, 0.30298844, 0.47277431, ..., 0.67711224, 0.55584904,\n",
       "         0.60472268],\n",
       "        [0.47070198, 0.44459997, 0.40016028, ..., 0.71193979, 0.49769816,\n",
       "         0.71876507],\n",
       "        ...,\n",
       "        [0.85229627, 0.60343336, 0.18287765, ..., 0.54555271, 0.77112271,\n",
       "         0.18843358],\n",
       "        [0.78101646, 0.68586084, 0.40700551, ..., 0.45177062, 0.63841605,\n",
       "         0.48128186],\n",
       "        [0.88757249, 0.75954519, 0.82721872, ..., 0.3422384 , 0.33609074,\n",
       "         0.84539856]]),\n",
       " 'pi_b': array([[[0.05132742],\n",
       "         [0.07509562],\n",
       "         [0.12113457],\n",
       "         ...,\n",
       "         [0.07518346],\n",
       "         [0.08126913],\n",
       "         [0.12123183]],\n",
       " \n",
       "        [[0.0913545 ],\n",
       "         [0.14356775],\n",
       "         [0.10223103],\n",
       "         ...,\n",
       "         [0.06793555],\n",
       "         [0.08658147],\n",
       "         [0.07851884]],\n",
       " \n",
       "        [[0.11315543],\n",
       "         [0.1192195 ],\n",
       "         [0.13030082],\n",
       "         ...,\n",
       "         [0.06984557],\n",
       "         [0.1072079 ],\n",
       "         [0.06889862]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.04138881],\n",
       "         [0.0680836 ],\n",
       "         [0.15788198],\n",
       "         ...,\n",
       "         [0.07643935],\n",
       "         [0.04868435],\n",
       "         [0.15613733]],\n",
       " \n",
       "        [[0.05611272],\n",
       "         [0.06787541],\n",
       "         [0.11855589],\n",
       "         ...,\n",
       "         [0.10840284],\n",
       "         [0.07463155],\n",
       "         [0.10218979]],\n",
       " \n",
       "        [[0.04997525],\n",
       "         [0.06455919],\n",
       "         [0.05638682],\n",
       "         ...,\n",
       "         [0.14873944],\n",
       "         [0.15057953],\n",
       "         [0.05437344]]]),\n",
       " 'pscore': array([0.12123183, 0.10223103, 0.1192195 , ..., 0.04138881, 0.11885694,\n",
       "        0.14873944])}"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "bandit_feedback_test"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'n_rounds': 10000,\n",
       " 'n_actions': 10,\n",
       " 'context': array([[ 0.43473651,  0.28892246, -0.80945479,  1.59442032,  0.86886285],\n",
       "        [ 0.40338143, -0.47894608, -0.06737703, -2.65932054, -0.83837731],\n",
       "        [ 2.47244458,  0.59674369,  0.61911426, -0.20459904,  0.14098079],\n",
       "        ...,\n",
       "        [ 1.42408759,  0.43832029,  1.60780305, -0.27684065, -1.41350158],\n",
       "        [-0.65257593, -1.4938017 , -0.66387424,  0.68710758, -1.25823339],\n",
       "        [-1.02877579,  0.40254317, -0.24025777,  0.22296652, -1.10929397]]),\n",
       " 'action_context': array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]),\n",
       " 'action': array([9, 4, 1, ..., 0, 2, 7]),\n",
       " 'position': None,\n",
       " 'reward': array([0, 0, 1, ..., 0, 0, 0]),\n",
       " 'expected_reward': array([[0.75538494, 0.29998174, 0.24536269, ..., 0.54558479, 0.67971713,\n",
       "         0.56415778],\n",
       "        [0.56771101, 0.83851521, 0.89249222, ..., 0.36474694, 0.34964153,\n",
       "         0.82760191],\n",
       "        [0.38798147, 0.31102483, 0.84585131, ..., 0.60051483, 0.40078146,\n",
       "         0.77725236],\n",
       "        ...,\n",
       "        [0.2211665 , 0.4608221 , 0.88164588, ..., 0.37853204, 0.55411957,\n",
       "         0.6891682 ],\n",
       "        [0.60607356, 0.61444185, 0.32843586, ..., 0.29771597, 0.67156368,\n",
       "         0.79549896],\n",
       "        [0.69531026, 0.66225066, 0.48318881, ..., 0.24025407, 0.74679925,\n",
       "         0.49011547]]),\n",
       " 'pi_b': array([[[0.05981305],\n",
       "         [0.14871479],\n",
       "         [0.16588062],\n",
       "         ...,\n",
       "         [0.09099679],\n",
       "         [0.06958573],\n",
       "         [0.08767863]],\n",
       " \n",
       "        [[0.08255928],\n",
       "         [0.04803396],\n",
       "         [0.04311859],\n",
       "         ...,\n",
       "         [0.12389627],\n",
       "         [0.12769639],\n",
       "         [0.0490939 ]],\n",
       " \n",
       "        [[0.10768438],\n",
       "         [0.12560196],\n",
       "         [0.0430975 ],\n",
       "         ...,\n",
       "         [0.0703961 ],\n",
       "         [0.10496265],\n",
       "         [0.0494352 ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.1323737 ],\n",
       "         [0.08196709],\n",
       "         [0.0353278 ],\n",
       "         ...,\n",
       "         [0.09663084],\n",
       "         [0.06801463],\n",
       "         [0.05191593]],\n",
       " \n",
       "        [[0.08607778],\n",
       "         [0.08464912],\n",
       "         [0.14998372],\n",
       "         ...,\n",
       "         [0.15948765],\n",
       "         [0.07551045],\n",
       "         [0.05893296]],\n",
       " \n",
       "        [[0.0604721 ],\n",
       "         [0.06460562],\n",
       "         [0.09242755],\n",
       "         ...,\n",
       "         [0.15024909],\n",
       "         [0.05455472],\n",
       "         [0.09115595]]]),\n",
       " 'pscore': array([0.08767863, 0.12247745, 0.12560196, ..., 0.1323737 , 0.14998372,\n",
       "        0.15024909])}"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (2) Off-Policy Learning\n",
    "After generating synthetic data, we now train some decision making policies.\n",
    "\n",
    "To train policies on logged bandit data, we use\n",
    "\n",
    "- `obp.policy.NNPolicyLearner` (Neural Network Policy Learner)\n",
    "- `obp.policy.IPWLearner`\n",
    "\n",
    "For `NN Learner`, we use \n",
    "- Direct Method (\"dm\")\n",
    "- InverseProbabilityWeighting (\"ipw\")\n",
    "- DoublyRobust (\"dr\") \n",
    "\n",
    "as its objective functions (`off_policy_objective`). \n",
    "\n",
    "For `IPW Learner`, we use `RandomForestClassifier` and *LogisticRegression* implemented in scikit-learn for base ML methods."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A policy is trained by maximizing an OPE estimator as an objective function as follows.\n",
    "\n",
    "$$ \\hat{\\pi} \\in \\arg \\max_{\\pi \\in \\Pi} \\hat{V} (\\pi; \\mathcal{D}_{tr}) - \\lambda \\cdot \\Omega (\\pi)  $$\n",
    "\n",
    "where $\\hat{V}(\\cdot; \\mathcal{D})$ is an off-policy objective and $\\mathcal{D}_{tr}$ is a training bandit dataset. $\\Omega (\\cdot)$ is a regularization term."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "# define NNPolicyLearner with DM as its objective function\n",
    "nn_dm = NNPolicyLearner(\n",
    "    n_actions=dataset.n_actions,\n",
    "    dim_context=dataset.dim_context,\n",
    "    off_policy_objective=\"dm\",\n",
    "    batch_size=64,\n",
    "    random_state=12345,\n",
    ")\n",
    "\n",
    "# train NNPolicyLearner on the training set of logged bandit data\n",
    "nn_dm.fit(\n",
    "    context=bandit_feedback_train[\"context\"],\n",
    "    action=bandit_feedback_train[\"action\"],\n",
    "    reward=bandit_feedback_train[\"reward\"],\n",
    ")\n",
    "\n",
    "# obtains action choice probabilities for the test set\n",
    "action_dist_nn_dm = nn_dm.predict_proba(\n",
    "    context=bandit_feedback_test[\"context\"]\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "q-func learning: 100%|██████████| 200/200 [00:32<00:00,  6.07it/s]\n",
      "policy learning:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1 0 0 ... 0 0 1]\n",
      "[ 0.5 -0.5 -0.5 ... -0.5 -0.5  0.5]\n",
      "Here\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "policy learning: 100%|██████████| 200/200 [01:16<00:00,  2.61it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "# define NNPolicyLearner with IPW as its objective function\n",
    "nn_ipw = NNPolicyLearner(\n",
    "    n_actions=dataset.n_actions,\n",
    "    dim_context=dataset.dim_context,\n",
    "    off_policy_objective=\"ipw\",\n",
    "    batch_size=64,\n",
    "    random_state=12345,\n",
    "    loss_translation = 0\n",
    ")\n",
    "\n",
    "# train NNPolicyLearner on the training set of logged bandit data\n",
    "nn_ipw.fit(\n",
    "    context=bandit_feedback_train[\"context\"],\n",
    "    action=bandit_feedback_train[\"action\"],\n",
    "    reward=bandit_feedback_train[\"reward\"],\n",
    "    pscore=bandit_feedback_train[\"pscore\"],\n",
    ")\n",
    "#MAKE A FOR LOOOP TO SUBTRACT LAMBDA FROM REWARD DIRECTLY??\n",
    "\n",
    "# obtains action choice probabilities for the test set\n",
    "action_dist_nn_ipw = nn_ipw.predict_proba(\n",
    "    context=bandit_feedback_test[\"context\"]\n",
    ")\n",
    "pred_actions = action_dist_nn_ipw [:,:,0]\n",
    "action = bandit_feedback_test[\"action\"]\n",
    "idx_tensor = torch.arange(action.shape[0], dtype=torch.long)\n",
    "pscore = bandit_feedback_test[\"pscore\"]\n",
    "rewards = bandit_feedback_test[\"reward\"]\n",
    "\n",
    "iw = pred_actions[idx_tensor, action] / pscore\n",
    "numerator = np.mean(iw * rewards)\n",
    "print(pred_actions[idx_tensor, action])\n",
    "print(numerator)\n",
    "\n",
    "\n",
    "#action = np.ndarray(np.amax(x) for x in pred_actions)\n",
    "#print(action)\n",
    "\n",
    "# current_pi = action_dist[:, :, 0].detach()\n",
    "#log_prob = torch.log(action_dist[:, :, 0])\n",
    "#idx_tensor = torch.arange(action.shape[0], dtype=torch.long) \n",
    "#iw = current_pi[idx_tensor, action] / pscore #here action is the index of max probabilities\n",
    "#numerator = actions/pscore[:,pred_action]\n",
    "#numerator = bandit_feedback_test[\"reward\"] * (action_dist_nn_ipw\n",
    "#0.7592709473199616\n",
    "#0.7619300315045386\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'loss_translation'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-0c0c851a3024>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# define NNPolicyLearner with IPW as its objective function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m nn_ipw = NNPolicyLearner(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mn_actions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdim_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moff_policy_objective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ipw\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'loss_translation'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# define NNPolicyLearner with IPW as its objective function\n",
    "nn_ipw = NNPolicyLearner(\n",
    "    n_actions=dataset.n_actions,\n",
    "    dim_context=dataset.dim_context,\n",
    "    off_policy_objective=\"ipw\",\n",
    "    batch_size=64,\n",
    "    random_state=12345,\n",
    ")\n",
    "print(bandit_feedback_train[\"reward\"] - 0)\n",
    "print(bandit_feedback_train[\"reward\"] - 0.5)\n",
    "\n",
    "# train NNPolicyLearner on the training set of logged bandit data\n",
    "nn_ipw.fit(\n",
    "    context=bandit_feedback_train[\"context\"],\n",
    "    action=bandit_feedback_train[\"action\"],\n",
    "    reward=bandit_feedback_train[\"reward\"],\n",
    "    pscore=bandit_feedback_train[\"pscore\"],\n",
    ")\n",
    "#MAKE A FOR LOOOP TO SUBTRACT LAMBDA FROM REWARD DIRECTLY??\n",
    "#PUT LOSS_TRANSLATION AS A HYPERPARAMETER IN THE BEGINNING OF THE FILE??\n",
    "\n",
    "# obtains action choice probabilities for the test set\n",
    "action_dist_nn_ipw = nn_ipw.predict_proba(\n",
    "    context=bandit_feedback_test[\"context\"]\n",
    ")\n",
    "pred_actions = action_dist_nn_ipw [:,:,0]\n",
    "action = bandit_feedback_test[\"action\"]\n",
    "idx_tensor = torch.arange(action.shape[0], dtype=torch.long)\n",
    "pscore = bandit_feedback_test[\"pscore\"]\n",
    "rewards = bandit_feedback_test[\"reward\"]\n",
    "\n",
    "iw = pred_actions[idx_tensor, action] / pscore\n",
    "numerator = np.mean(iw * rewards)\n",
    "print(pred_actions[idx_tensor, action])\n",
    "print(numerator)\n",
    "# log_prob = torch.log(torch.from_numpy(action_dist_nn_ipw[:, :, 0]))\n",
    "\n",
    "# current_pi = torch.from_numpy(action_dist_nn_ipw)\n",
    "# iw = current_pi[idx_tensor, action]\n",
    "# estimated_policy_grad_1 = iw * (rewards - 0.5)\n",
    "# estimated_policy_grad_2 = iw * (rewards - 0.2)\n",
    "# estimated_policy_grad_1 *= log_prob[idx_tensor, action]\n",
    "# estimated_policy_grad_2 *= log_prob[idx_tensor, action]\n",
    "# print(estimated_policy_grad_1, np.mean((estimated_policy_grad_1).numpy()))\n",
    "# print(estimated_policy_grad_2, np.mean((estimated_policy_grad_2).numpy()))\n",
    "\n",
    "\n",
    "\n",
    "#action = np.ndarray(np.amax(x) for x in pred_actions)\n",
    "#print(action)\n",
    "\n",
    "# current_pi = action_dist[:, :, 0].detach()\n",
    "#log_prob = torch.log(action_dist[:, :, 0])\n",
    "#idx_tensor = torch.arange(action.shape[0], dtype=torch.long) \n",
    "#iw = current_pi[idx_tensor, action] / pscore #here action is the index of max probabilities\n",
    "#numerator = actions/pscore[:,pred_action]\n",
    "#numerator = bandit_feedback_test[\"reward\"] * (action_dist_nn_ipw)\n",
    "#0.7592709473199616"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "policy learning:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1 0 0 ... 0 0 1]\n",
      "[ 0.5 -0.5 -0.5 ... -0.5 -0.5  0.5]\n",
      "[1 0 0 ... 0 0 1]\n",
      "[ 0.5 -0.5 -0.5 ... -0.5 -0.5  0.5]\n",
      "Here\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "policy learning: 100%|██████████| 200/200 [01:07<00:00,  2.97it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1.5660349e-03 1.5724966e-06 1.7662393e-04 ... 6.0777864e-05 1.2138274e-06\n",
      " 2.8558900e-06]\n",
      "0.7619300315045386\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# define NNPolicyLearner with DR as its objective function\n",
    "nn_dr = NNPolicyLearner(\n",
    "    n_actions=dataset.n_actions,\n",
    "    dim_context=dataset.dim_context,\n",
    "    off_policy_objective=\"dr\",\n",
    "    batch_size=64,\n",
    "    random_state=12345,\n",
    ")\n",
    "\n",
    "# train NNPolicyLearner on the training set of logged bandit data\n",
    "nn_dr.fit(\n",
    "    context=bandit_feedback_train[\"context\"],\n",
    "    action=bandit_feedback_train[\"action\"],\n",
    "    reward=bandit_feedback_train[\"reward\"],\n",
    "    pscore=bandit_feedback_train[\"pscore\"],\n",
    ")\n",
    "\n",
    "# obtains action choice probabilities for the test set\n",
    "action_dist_nn_dr = nn_dr.predict_proba(\n",
    "    context=bandit_feedback_test[\"context\"]\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "q-func learning: 100%|██████████| 200/200 [00:30<00:00,  6.64it/s]\n",
      "policy learning:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1 0 0 ... 0 0 1]\n",
      "[ 0.5 -0.5 -0.5 ... -0.5 -0.5  0.5]\n",
      "Here\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "policy learning: 100%|██████████| 200/200 [01:19<00:00,  2.51it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# define IPWLearner with Logistic Regression as its base ML model\n",
    "ipw_lr = IPWLearner(\n",
    "    n_actions=dataset.n_actions,\n",
    "    base_classifier=LogisticRegression(C=100, random_state=12345)\n",
    ")\n",
    "\n",
    "# train IPWLearner on the training set of logged bandit data\n",
    "ipw_lr.fit(\n",
    "    context=bandit_feedback_train[\"context\"],\n",
    "    action=bandit_feedback_train[\"action\"],\n",
    "    reward=bandit_feedback_train[\"reward\"],\n",
    "    pscore=bandit_feedback_train[\"pscore\"]\n",
    ")\n",
    "\n",
    "# obtains action choice probabilities for the test set\n",
    "action_dist_ipw_lr = ipw_lr.predict(\n",
    "    context=bandit_feedback_test[\"context\"]\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# define IPWLearner with Random Forest as its base ML model\n",
    "ipw_rf = IPWLearner(\n",
    "    n_actions=dataset.n_actions,\n",
    "    base_classifier=RandomForest(\n",
    "        n_estimators=30, min_samples_leaf=10, random_state=12345\n",
    "    )\n",
    ")\n",
    "\n",
    "# train IPWLearner on the training set of logged bandit data\n",
    "ipw_rf.fit(\n",
    "    context=bandit_feedback_train[\"context\"],\n",
    "    action=bandit_feedback_train[\"action\"],\n",
    "    reward=bandit_feedback_train[\"reward\"],\n",
    "    pscore=bandit_feedback_train[\"pscore\"],\n",
    "    l = 0\n",
    ")\n",
    "\n",
    "# obtains action choice probabilities for the test set\n",
    "action_dist_ipw_rf = ipw_rf.predict(\n",
    "    context=bandit_feedback_test[\"context\"]\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "fit() got an unexpected keyword argument 'l'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a055b30277f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# train IPWLearner on the training set of logged bandit data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m ipw_rf.fit(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbandit_feedback_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"context\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbandit_feedback_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"action\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: fit() got an unexpected keyword argument 'l'"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# define Uniform Random Policy as a baseline evaluation policy\n",
    "random = Random(n_actions=dataset.n_actions,)\n",
    "\n",
    "# compute the action choice probabilities for the test set\n",
    "action_dist_random = random.compute_batch_action_dist(\n",
    "    n_rounds=bandit_feedback_test[\"n_rounds\"]\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# action_dist is a probability distribution over actions (can be deterministic)\n",
    "action_dist_ipw_lr[:, :, 0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (3) Evaluation of Off-Policy Learners\n",
    "Our final step is the evaluation and comparison of the off-policy learners.\n",
    "\n",
    "With synthetic data, we can calculate the policy value of the off-policy learners as follows. \n",
    "\n",
    "$$V(\\pi_e) \\approx \\frac{1}{|\\mathcal{D}_{te}|} \\sum_{i=1}^{|\\mathcal{D}_{te}|} \\mathbb{E}_{a \\sim \\pi_e(a|x_i)} [q(x_i, a)], \\; \\, where \\; \\, q(x,a) := \\mathbb{E}_{r \\sim p(r|x,a)} [r]$$\n",
    "\n",
    "where $\\mathcal{D}_{te}$ is the test set of logged bandit data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# we calculate the policy values of the trained policies based on the expected rewards of the test data\n",
    "policy_names = [\n",
    "    \"NN Policy Learner with DM\",\n",
    "    \"NN Policy Learner with IPW\",\n",
    "    \"NN Policy Learner with DR\",\n",
    "    \"IPW Learner with Logistic Regression\",\n",
    "    \"IPW Learner with Random Forest\",\n",
    "    \"Unifrom Random\"\n",
    "]\n",
    "action_dist_list = [\n",
    "    action_dist_nn_dm,\n",
    "    action_dist_nn_ipw,\n",
    "    action_dist_nn_dr,\n",
    "    action_dist_ipw_lr,\n",
    "    action_dist_ipw_rf,\n",
    "    action_dist_random\n",
    "]\n",
    "\n",
    "for name, action_dist in zip(policy_names, action_dist_list):\n",
    "    true_policy_value = dataset.calc_ground_truth_policy_value(\n",
    "        expected_reward=bandit_feedback_test[\"expected_reward\"],\n",
    "        action_dist=action_dist,\n",
    "    )\n",
    "    print(f'policy value of {name}: {true_policy_value}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "policy value of NN Policy Learner with DM: 0.7862505830999654\n",
      "policy value of NN Policy Learner with IPW: 0.7606162025424541\n",
      "policy value of NN Policy Learner with DR: 0.7732793867972861\n",
      "policy value of IPW Learner with Logistic Regression: 0.7933299733929567\n",
      "policy value of IPW Learner with Random Forest: 0.7050722711915117\n",
      "policy value of Unifrom Random: 0.49992528545607745\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In fact, `IPWLearner` with `LogisticRegression` seems to be the best in this simple setting."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can iterate the above process several times to get more reliable results.\n",
    "\n",
    "Please see [../examples/opl](../opl) for a more sophisticated example of the evaluation of off-policy learners with synthetic bandit data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7adc034d2bae42754621dfef56fb826d586cc3ae89b4958f3a6eaffc035e1bc9"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}